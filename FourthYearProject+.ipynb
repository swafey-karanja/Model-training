{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swafey-karanja/Model-training/blob/main/FourthYearProject%2B.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeANMo5VPYYo"
      },
      "source": [
        "# **VET-ASSISTANT LLM**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu96hl0nOwUk"
      },
      "source": [
        "**Installing dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un-Qup9eNRq3",
        "outputId": "c5540748-b0c6-445f-842b-b17a926769ce"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting gradientai\n",
            "  Downloading gradientai-1.8.0-py3-none-any.whl (296 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/296.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━\u001b[0m \u001b[32m286.7/296.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aenum>=3.1.11 (from gradientai)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.5 (from gradientai)\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
            "Installing collected packages: aenum, pydantic, gradientai\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.3\n",
            "    Uninstalling pydantic-2.6.3:\n",
            "      Successfully uninstalled pydantic-2.6.3\n",
            "Successfully installed aenum-3.1.15 gradientai-1.8.0 pydantic-1.10.14\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
            "  Downloading langchain_community-0.0.27-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m63.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n",
            "  Downloading langchain_core-0.1.30-py3-none-any.whl (256 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.27 langchain-core-0.1.30 langchain-text-splitters-0.0.1 langsmith-0.1.22 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 typing-inspect-0.9.0\n",
            "Collecting gradient_haystack\n",
            "  Downloading gradient_haystack-0.4.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradient_haystack) (1.8.0)\n",
            "Collecting haystack-ai (from gradient_haystack)\n",
            "  Downloading haystack_ai-2.0.0b8-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (3.1.15)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (1.10.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.0.7)\n",
            "Collecting boilerpy3 (from haystack-ai->gradient_haystack)\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting haystack-bm25 (from haystack-ai->gradient_haystack)\n",
            "  Downloading haystack_bm25-1.0.2-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.1.3)\n",
            "Collecting lazy-imports (from haystack-ai->gradient_haystack)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.2.1)\n",
            "Collecting openai>=1.1.0 (from haystack-ai->gradient_haystack)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (1.5.3)\n",
            "Collecting posthog (from haystack-ai->gradient_haystack)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (6.0.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (4.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai->gradient_haystack)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai->gradient_haystack) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack) (2023.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack) (2.31.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai->gradient_haystack)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai->gradient_haystack)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai->gradient_haystack) (3.3.2)\n",
            "Installing collected packages: monotonic, lazy-imports, haystack-bm25, h11, boilerpy3, backoff, posthog, httpcore, httpx, openai, haystack-ai, gradient_haystack\n",
            "Successfully installed backoff-2.2.1 boilerpy3-1.0.7 gradient_haystack-0.4.0 h11-0.14.0 haystack-ai-2.0.0b8 haystack-bm25-1.0.2 httpcore-1.0.4 httpx-0.27.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.13.3 posthog-3.5.0\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.41.0-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.8/258.8 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.41.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradientai --upgrade\n",
        "!pip install langchain\n",
        "!pip install -U gradient_haystack\n",
        "!pip install regex\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install wandb\n",
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ids19xUiPP_A"
      },
      "source": [
        "## **1.Loading the Dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWjkVgavPkjM",
        "outputId": "50db473a-0ea7-4964-dff6-26fea9cc331c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset Size: 1412\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# URL of the online repository where the dataset is hosted\n",
        "dataset_url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/NewData.json\"\n",
        "\n",
        "# Make an HTTP GET request to fetch the dataset\n",
        "response = requests.get(dataset_url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Load the dataset from the response content\n",
        "    train_dataset = json.loads(response.text)\n",
        "\n",
        "    # Print the size of the dataset\n",
        "    print(\"Dataset Size:\", len(train_dataset))\n",
        "else:\n",
        "    # Print an error message if the request failed\n",
        "    print(\"Failed to fetch dataset. Status code:\", response.status_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnzGaOMVpGf4"
      },
      "source": [
        "**Break the data into batches**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYzINIs6pErW",
        "outputId": "31d12f0e-9c64-4237-d08d-1ea3b7e84c4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batches size\n",
            "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 12]\n"
          ]
        }
      ],
      "source": [
        "def divide_into_Batches(number, chunk_size):  # Define a function to divide a number into chunks of a given size\n",
        "    Batches = []\n",
        "    while number > 0:\n",
        "        if number >= chunk_size:\n",
        "            Batches.append(chunk_size)\n",
        "            number -= chunk_size\n",
        "        else:\n",
        "            Batches.append(number)\n",
        "            break\n",
        "\n",
        "    return Batches\n",
        "\n",
        "Batches = divide_into_Batches(len(train_dataset), 100)  # Divide the dataset into chunks of 100 samples each\n",
        "print(\"Batches size\")\n",
        "print(Batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apmpbpeppW2T"
      },
      "source": [
        "## **2.Load the Base Model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-W4W_s3pySl"
      },
      "source": [
        "Loading the Nous-Hermes-Llama2-13b base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPy0iC61pj1o"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"oaiYItHbvKffOP1hTUj1463fcv0jVVHR\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOarEjaEpvWX",
        "outputId": "77d9849f-e7a0-4dbf-ad8a-a996e39bc8ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available Base Models\n",
            "\t <gradientai._base_model.BaseModel object at 0x7a52e3c9e6b0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7a52e3c9e5f0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7a52e3c9ebc0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7a52e3c9ee90>\n",
            "\t <gradientai._base_model.BaseModel object at 0x7a52e3c9eb30>\n",
            "\n",
            "Base Model Chosen : <gradientai._base_model.BaseModel object at 0x7a52e3c9edd0>\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import os\n",
        "from gradientai import Gradient\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"oaiYItHbvKffOP1hTUj1463fcv0jVVHR\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\"\n",
        "\n",
        "gradient =  Gradient()\n",
        "\n",
        "print(\"Available Base Models\")\n",
        "for i in gradient.list_models(only_base=True):\n",
        "    print(\"\\t\",i)\n",
        "\n",
        "base_model_id = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
        "base_model_name = \"nous-hermes2\"\n",
        "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\") # base model Nous-Hermes-Llama2-13b\n",
        "\n",
        "print(\"\\nBase Model Chosen :\", base_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G2WHiZMvtxGg"
      },
      "source": [
        "##**3. Creating a Model Adapter**\n",
        "\n",
        "* Adapters are small, lightweight modules inserted between\n",
        "existing layers of a pre-trained LLM. They act like \"add-ons\" that focus on learning task-specific information without modifying the core knowledge captured in the original model.\n",
        "\n",
        "* The addapter server as the object that we are going to fine tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKK8fwv5t_mX",
        "outputId": "b7114550-769c-4ca2-856e-b8a34fd50e61"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model id                : cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model\n",
            "Fine tune model Name         : Llama2-13b/Vet-assistant-Newdata\n",
            "Fine tune model adapter id   : 9eb593b3-747e-403e-8f19-06113e4a9a6b_model_adapter\n",
            "\n",
            "\n",
            "\n",
            "Size of object in memory, in bytes. 56\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'_api_instance': <gradientai.openapi.client.api.models_api.ModelsApi at 0x7a52e3c9e3b0>,\n",
              " '_id': '9eb593b3-747e-403e-8f19-06113e4a9a6b_model_adapter',\n",
              " '_workspace_id': 'c571b959-4ce8-474a-b3f3-398c7b347c57_workspace',\n",
              " '_async_semaphore': <asyncio.locks.Semaphore object at 0x7a52e3c9f2e0 [unlocked, value:8]>,\n",
              " '_base_model_id': 'cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model',\n",
              " '_name': 'Llama2-13b/Vet-assistant-Newdata'}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "our_finetune_model_name=\"Llama2-13b/Vet-assistant-Newdata\"\n",
        "Fine_Tune__adapter = base_model.create_model_adapter(\n",
        "        name=our_finetune_model_name, # base mode nous hermis\n",
        "        learning_rate=0.00005, #  Determines how fast a model updates its knowledge during fine-tuning.\n",
        "        rank=8,  # Dimensionality\n",
        "\n",
        "    )\n",
        "\n",
        "# default hyperparameters Frozen\n",
        "hyperparameters = {\n",
        "                  \"block_size\": 1024,\n",
        "                  \"model_max_length\": 2048,\n",
        "                  \"padding\": \"right\",\n",
        "                  \"use_flash_attention_2\": False,\n",
        "                  \"disable_gradient_checkpointing\": False,\n",
        "                  \"logging_steps\": -1,\n",
        "                  \"evaluation_strategy\": \"epoch\",\n",
        "                  \"save_total_limit\": 1,\n",
        "                  \"save_strategy\": \"epoch\",\n",
        "                  \"auto_find_batch_size\": False,\n",
        "                  \"mixed_precision\": \"fp16\",\n",
        "                  \"epochs\": 3,\n",
        "                  \"batch_size\": 100,\n",
        "                  \"warmup_ratio\": 0.1,\n",
        "                  \"gradient_accumulation\": 1,\n",
        "                  \"optimizer\": \"adamw_torch\",\n",
        "                  \"scheduler\": \"linear\",\n",
        "                  \"weight_decay\": 0,\n",
        "                  \"max_grad_norm\": 1,\n",
        "                  \"seed\": 42,\n",
        "                  \"apply_chat_template\": False,\n",
        "                  \"quantization\": \"int4\",\n",
        "                  \"target_modules\": \"\",\n",
        "                  \"merge_adapter\": False,\n",
        "                  \"peft\": True,\n",
        "                  \"lora_r\": 16,\n",
        "                  \"lora_alpha\": 32,\n",
        "                  \"lora_dropout\": 0.05\n",
        "  }\n",
        "\n",
        "\n",
        "print(f\"Base model id                : {Fine_Tune__adapter._base_model_id}\")\n",
        "print(f\"Fine tune model Name         : { Fine_Tune__adapter.name}\")\n",
        "print(f\"Fine tune model adapter id   : {Fine_Tune__adapter.id}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Size of object in memory, in bytes.\", Fine_Tune__adapter.__format__.__sizeof__()) # Size of object in memory, in bytes.\n",
        "Fine_Tune__adapter.__dict__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPU-mOK0uzfF"
      },
      "source": [
        "## **4. Fine Tuning The Adaptor**\n",
        "\n",
        "For this case we will be performing Laura-based finetuning which means that we are freezing about 99% of the layers and then finetuning an adapter on top of it.\n",
        "\n",
        "LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models with billions of parameters\n",
        "\n",
        " LoRA proposes to freeze pre-trained model weights and inject trainable layers (rank-decomposition matrices) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don't need to be computed for most model weights\n",
        "\n",
        "\n",
        "why LoRA finetuning:\n",
        "\n",
        "1. Faster Training: Since only the added task-specific layers are trained while the pre-trained model's parameters remain frozen, the fine-tuning process is generally faster compared to training a model from scratch\n",
        "2. Computation requirements are lower. We could create a full fine-tuned model in a 2080 Ti with 11 GB of VRAM!\n",
        "3. Trained weights are  much smaller. Because the original model is frozen and we inject new layers to be trained\n",
        "\n",
        " [for more info](https://huggingface.co/blog/lora)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhFoAaE5vPAk",
        "outputId": "8b780e52-e029-4fd8-c51b-eac8a710fa7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Model id:  9eb593b3-747e-403e-8f19-06113e4a9a6b_model_adapter\n",
            "================================================================\n",
            "\n",
            "Fine tuning . . .\n",
            "\n",
            "Fine-tuning the model, iteration 1\n",
            "Batch 1 range: 0 : 100\n",
            "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=18044.285\n",
            "Batch 2 range: 100 : 200\n",
            "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=11382.07\n",
            "Batch 3 range: 200 : 300\n",
            "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=10693.537\n",
            "Batch 4 range: 300 : 400\n",
            "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=12173.715\n",
            "Batch 5 range: 400 : 500\n",
            "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=14048.294\n",
            "Batch 6 range: 500 : 600\n",
            "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=18032.88\n",
            "Batch 7 range: 600 : 700\n",
            "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=18114.45\n",
            "Batch 8 range: 700 : 800\n",
            "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=17403.727\n",
            "Batch 9 range: 800 : 900\n",
            "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=11644.548\n",
            "Batch 10 range: 900 : 1000\n",
            "\t Batch 10 Evaluation : number_of_trainable_tokens=10560 sum_loss=9125.041\n",
            "Batch 11 range: 1000 : 1100\n",
            "\t Batch 11 Evaluation : number_of_trainable_tokens=9856 sum_loss=6480.017\n",
            "Batch 12 range: 1100 : 1200\n",
            "\t Batch 12 Evaluation : number_of_trainable_tokens=8558 sum_loss=6001.1772\n",
            "Batch 13 range: 1200 : 1300\n",
            "\t Batch 13 Evaluation : number_of_trainable_tokens=5507 sum_loss=5596.208\n",
            "Batch 14 range: 1300 : 1400\n",
            "\t Batch 14 Evaluation : number_of_trainable_tokens=5389 sum_loss=5130.1587\n",
            "Batch 15 range: 1400 : 1412\n",
            "\t Batch 15 Evaluation : number_of_trainable_tokens=999 sum_loss=943.2067\n",
            "Fine-tuning the model, iteration 2\n",
            "Batch 1 range: 0 : 100\n",
            "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=6583.622\n",
            "Batch 2 range: 100 : 200\n",
            "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=5588.251\n",
            "Batch 3 range: 200 : 300\n",
            "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=6237.9385\n",
            "Batch 4 range: 300 : 400\n",
            "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=8792.539\n",
            "Batch 5 range: 400 : 500\n",
            "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=9494.428\n",
            "Batch 6 range: 500 : 600\n",
            "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=11063.338\n",
            "Batch 7 range: 600 : 700\n",
            "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=12547.139\n",
            "Batch 8 range: 700 : 800\n",
            "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=9760.527\n",
            "Batch 9 range: 800 : 900\n",
            "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=5075.002\n",
            "Batch 10 range: 900 : 1000\n",
            "\t Batch 10 Evaluation : number_of_trainable_tokens=10560 sum_loss=5382.378\n",
            "Batch 11 range: 1000 : 1100\n",
            "\t Batch 11 Evaluation : number_of_trainable_tokens=9856 sum_loss=3513.8984\n",
            "Batch 12 range: 1100 : 1200\n",
            "\t Batch 12 Evaluation : number_of_trainable_tokens=8558 sum_loss=4633.2104\n",
            "Batch 13 range: 1200 : 1300\n",
            "\t Batch 13 Evaluation : number_of_trainable_tokens=5507 sum_loss=3977.7642\n",
            "Batch 14 range: 1300 : 1400\n",
            "\t Batch 14 Evaluation : number_of_trainable_tokens=5389 sum_loss=3003.7207\n",
            "Batch 15 range: 1400 : 1412\n",
            "\t Batch 15 Evaluation : number_of_trainable_tokens=999 sum_loss=388.2944\n"
          ]
        }
      ],
      "source": [
        "print(f\"Our Model id:  {Fine_Tune__adapter.id}\")\n",
        "num_epochs = 2  # num_epochs is the number of times you fine-tune the model # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
        "count = 0\n",
        "print(\"================================================================\\n\")\n",
        "print(\"Fine tuning . . .\\n\")\n",
        "while count < num_epochs:\n",
        "    print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
        "    s = 0\n",
        "    n = 1\n",
        "    for Batch in Batches:\n",
        "        print(f\"Batch {n} range: {s} : {(s + Batch)}\")\n",
        "\n",
        "        # Try to fine-tune the model with the chunk of samples,\n",
        "        while True:\n",
        "            try:\n",
        "                metric = Fine_Tune__adapter.fine_tune(samples=train_dataset[s: s + Batch])\n",
        "                print(f\"\\t Batch {n} Evaluation :\", metric)\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "        s += Batch\n",
        "        n += 1\n",
        "    count = count + 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JH8ghczL_Xy6"
      },
      "source": [
        "## **5.Model Inference**\n",
        "\n",
        "\"model inference\" typically refers to the process of using a trained model to make predictions on new, unseen data. Fine-tuning a model involves taking a pre-trained model and further training it on a specific task or dataset to improve its performance.\n",
        "\n",
        "When fine-tuning a model, the process of inference remains the same as with any other trained model. Once the fine-tuning is complete, you can use the model to make predictions on new data by passing the data through the model and obtaining the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsGJw4hw_8O5"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMChain    # Import the LLMChain class for building LLM-based workflows\n",
        "from langchain.llms import GradientLLM   # Import the GradientLLM class for interacting with Gradient AI's API\n",
        "from langchain.prompts import PromptTemplate # Import the PromptTemplate class for defining how to prompt the LLM\n",
        "import gradientai\n",
        "import os # Import the os module for potential file system interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vbg5iMnAA1-"
      },
      "outputs": [],
      "source": [
        "Fine_Tune__adapter_ID = \"9eb593b3-747e-403e-8f19-06113e4a9a6b_model_adapter\"\n",
        "#Fine_Tune__adapter_ID = Fine_Tune__adapter.id\n",
        "#  creating a GradientLLM object\n",
        "llm = GradientLLM(\n",
        "    model=Fine_Tune__adapter_ID,\n",
        "    model_kwargs=dict(\n",
        "        max_generated_token_count=128, # Adjust how your model generates completions\n",
        "        temperature = 0.7, # randomness\n",
        "        top_k=50 # Restricts the model to pick from k most likely words,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVCG5MR7Au16"
      },
      "source": [
        "### **Formatting Prompts**\n",
        "\n",
        "The model follows the Alpaca prompt format which provides a structured way to input information to the model, guiding it on what kind of output is desired."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fzevoH6vBLXc"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"### Instruction: {Instruction} \\n\\n### Response:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"Instruction\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg0GwPNHBmjo"
      },
      "source": [
        "#### *Example* *inputs* *and* *corresponding* *outputs*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcTdHlRZBcN7",
        "outputId": "16b139f7-3865-4405-bc54-adaea09c403d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question :\n",
            " How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer :\n",
            " Clostridial Diseases, caused by various Clostridium species, can impact milk production, reproductive performance, and overall well-being in cows. \n",
            "\n",
            "\n",
            "Question :\n",
            " what are some of the risk factors associated with lameness in dairy cows?\n",
            "Answer :\n",
            " Risk factors for lameness in dairy cows include poor hoof health, injury, and exposure to wet and muddy conditions. \n",
            "\n",
            "\n",
            "Question :\n",
            " How dose lameness impact mill production, reproduction and overall well-being in cows\n",
            "Answer :\n",
            " Lameness can negatively impact mill production by reducing mobility and productivity. It can also affect reproduction by causing discomfort during mating and potentially reducing fertility. Overall well-being is also compromised, as lame cows may experience pain and discomfort. \n",
            "\n",
            "\n",
            "Question :\n",
            " What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \n",
            "Answer :\n",
            " Diseases such as mastitis and foot rot are common in dairy small ruminants. Management practices like proper sanitation, providing clean water, and rotating pastures can mitigate their impact. \n",
            "\n",
            "\n",
            "Question :\n",
            " what specific health managment strategies should be implemented to prevent or treat common cow diseases?\n",
            "Answer :\n",
            " Implementing specific health management strategies to prevent or treat common cow diseases involves providing clean water and nutritious food, maintaining a healthy herd through vaccination and deworming, and promptly isolating any sick animals. \n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "Question1 = \"How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\"\n",
        "Question2 = \"what are some of the risk factors associated with lameness in dairy cows?\"\n",
        "Question3 = \"How dose lameness impact mill production, reproduction and overall well-being in cows\"\n",
        "Question4 = \"What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \"\n",
        "Question5 = \"what specific health managment strategies should be implemented to prevent or treat common cow diseases?\"\n",
        "\n",
        "\n",
        "print(\"Question :\\n\", Question1)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question1}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question2)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question2}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question3)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question3}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question4)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question4}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question5)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question5}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCh6YPY-DPh8"
      },
      "source": [
        "## **6.Model Evaluation**\n",
        "\n",
        "Here we are using two popular automatic evaluation metrics to assess the performance of your LLM:\n",
        "\n",
        "* BLEU score: This metric calculates the n-gram precision between the generated response and the reference response\n",
        "\n",
        "* ROUGE score: This metric measures the overlap in word n-grams and longest common subsequences between the generated response and the reference response.\n",
        "\n",
        "BLEU and ROUGE scores are calculated to compare the generated response with the target response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxVdOqQJEZNS"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHfeZ8FfEbXp",
        "outputId": "98ddbc9f-ecf8-4431-d1c7-d8fa78605255"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " =================================== Evaluation =================================== \n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " How does technology contribute to advancements in animal husbandry?\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Technology in animal husbandry includes innovations like automated feeding systems, precision breeding techniques, and health monitoring devices. These advancements enhance efficiency, reduce costs, and improve overall management practices.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Technology in animal husbandry involves monitoring animals, automating tasks, and using sensors to optimize feeding, health, and overall management. Automated feeding systems, GPS tracking, and data analytics are examples of technology used to enhance efficiency and effectiveness in animal husbandry.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.36666666171666673\n",
            "\tROUGE-2 F1 Score: 0.15624999512207047\n",
            "\tROUGE-L F1 Score: 0.36666666171666673\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Elaborate on the challenges faced in modern animal husbandry practices.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Modern animal husbandry faces challenges such as disease management, ethical concerns, and environmental impact. Balancing productivity with animal welfare and sustainability is an ongoing challenge for practitioners in the field.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Modern animal husbandry faces challenges such as disease management, environmental concerns, and public perception. Exploring these issues helps us understand the complexities of managing a modern animal husbandry operation.\n",
            "\n",
            "BLEU Score: 7.850707993042515e-232\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.47272726772892565\n",
            "\tROUGE-2 F1 Score: 0.32142856643494905\n",
            "\tROUGE-L F1 Score: 0.4363636313652893\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Discuss the role of nutrition in animal husbandry.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Nutrition plays a crucial role in animal husbandry as it directly impacts the health, growth, and productivity of livestock. Properly balanced diets ensure optimal development and efficient utilization of nutrients for various purposes, such as milk and meat production.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LLM RESPONSE:\n",
            " Nutrition plays a critical role in animal husbandry. Proper feeding ensures optimal growth and health.\n",
            "\n",
            "BLEU Score: 9.257324954728539e-232\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.3599999958\n",
            "\tROUGE-2 F1 Score: 0.19230768837278112\n",
            "\tROUGE-L F1 Score: 0.3599999958\n",
            "\n",
            "AverageBLEU Score: 1.7108032947771055e-231\n",
            "Average ROUGE Scores for 3 samples\n",
            "\tAverage ROUGE-1 F1 Score: 0.3997979750818641\n",
            "\tAverage ROUGE-2 F1 Score: 0.2233287499766002\n",
            "\tAverageROUGE-L F1 Score: 0.38767676296065195\n",
            "\n",
            " ---------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def compute_rouge_scores(hypotheses, references):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_bleu_score(target_response, llm_responses):\n",
        "    bleu_score = corpus_bleu([target_response.split()], [llm_responses.split()])  # Calculate BLEU score\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def Find_Instruction(input_pattern, input_string):\n",
        "    matches = re.findall(input_pattern, input_string, re.DOTALL)\n",
        "\n",
        "    # If there are matches, extract the first one\n",
        "    extracted_string = None\n",
        "    if matches:\n",
        "        extracted_string = matches[0]\n",
        "\n",
        "    return extracted_string\n",
        "\n",
        "\n",
        "def Evaluate(Sample=None, count=0):\n",
        "    print(\"\\n =================================== Evaluation =================================== \")\n",
        "    input_pattern = r'<s>### Instruction:\\n(.*?) \\n'\n",
        "    response_pattern = r'Response:\\n(.*?)</s>'\n",
        "    bleu_scoreS = []\n",
        "    rouge_scoreS = []\n",
        "\n",
        "    if count != 0:\n",
        "        iteration = count - 1\n",
        "    else:\n",
        "        iteration = count\n",
        "\n",
        "    while iteration >= 0:\n",
        "\n",
        "        input_query = Find_Instruction(input_pattern, Sample[iteration][\"inputs\"])\n",
        "        target_response = Find_Instruction(response_pattern, Sample[iteration][\"inputs\"])\n",
        "\n",
        "        if input_query and target_response is not None:\n",
        "            print(\"\\n ---------------------------------------------------------------\")\n",
        "            print(\"INPUT QUERY:\\n\", input_query)\n",
        "            print(\"\\nTARGET RESPONSE:\\n\", target_response)\n",
        "\n",
        "            llm_responses = llm_chain.run(Instruction=f\"{input_query}\")\n",
        "            print(\"\\nLLM RESPONSE:\\n\", llm_responses)\n",
        "\n",
        "            rouge_scores = compute_rouge_scores(llm_responses, target_response)\n",
        "\n",
        "            bleu_score = compute_bleu_score(target_response, llm_responses)\n",
        "            print(\"\\nBLEU Score:\", bleu_score)\n",
        "            print(\"ROUGE Scores:\")\n",
        "            print(\"\\tROUGE-1 F1 Score:\", rouge_scores[\"rouge-1\"][\"f\"])\n",
        "            print(\"\\tROUGE-2 F1 Score:\", rouge_scores[\"rouge-2\"][\"f\"])\n",
        "            print(\"\\tROUGE-L F1 Score:\", rouge_scores[\"rouge-l\"][\"f\"])\n",
        "            rouge_scoreS.append((rouge_scores[\"rouge-1\"][\"f\"], rouge_scores[\"rouge-2\"][\"f\"], rouge_scores[\"rouge-l\"][\"f\"]))\n",
        "            bleu_scoreS.append(bleu_score)\n",
        "\n",
        "\n",
        "        iteration -= 1\n",
        "\n",
        "    if count > 0:\n",
        "        rouge_scores1 = 0\n",
        "        rouge_scores2 = 0\n",
        "        rouge_scores3 = 0\n",
        "        bleu_scoreA = 0\n",
        "\n",
        "        for i in bleu_scoreS:\n",
        "            bleu_scoreA += i\n",
        "        for i in rouge_scoreS:\n",
        "            rouge_scores1 += i[0]\n",
        "            rouge_scores2 += i[1]\n",
        "            rouge_scores3 += i[2]\n",
        "\n",
        "        print(\"\\nAverageBLEU Score:\", bleu_scoreA)\n",
        "        print(f\"Average ROUGE Scores for {count} samples\")\n",
        "        print(\"\\tAverage ROUGE-1 F1 Score:\", rouge_scores1 / count)\n",
        "        print(\"\\tAverage ROUGE-2 F1 Score:\", rouge_scores2 / count)\n",
        "        print(\"\\tAverageROUGE-L F1 Score:\", rouge_scores3 / count)\n",
        "\n",
        "    print(\"\\n ---------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "Evaluate(Sample=train_dataset, count=3)  # one sample evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oBkpjyuFZlu"
      },
      "source": [
        " ## **7. Intergrating  Retreival-Augmented Generation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4f4FrLhGJ7V",
        "outputId": "411e9c0d-92c8-4d51-e743-aa05c36e5400"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradient_haystack==0.2.0\n",
            "  Downloading gradient_haystack-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradient_haystack==0.2.0) (1.8.0)\n",
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.10/dist-packages (from gradient_haystack==0.2.0) (2.0.0b8)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (3.1.15)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (1.10.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.0.7)\n",
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.0.7)\n",
            "Requirement already satisfied: haystack-bm25 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.1.3)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.2.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.5.3)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai->gradient_haystack==0.2.0) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack==0.2.0) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack==0.2.0) (2023.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.31.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai->gradient_haystack==0.2.0) (3.3.2)\n",
            "Installing collected packages: gradient_haystack\n",
            "  Attempting uninstall: gradient_haystack\n",
            "    Found existing installation: gradient-haystack 0.4.0\n",
            "    Uninstalling gradient-haystack-0.4.0:\n",
            "      Successfully uninstalled gradient-haystack-0.4.0\n",
            "Successfully installed gradient_haystack-0.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradient_haystack==0.2.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE3KAEc6GLVK"
      },
      "outputs": [],
      "source": [
        "from gradient_haystack.embedders.gradient_document_embedder import GradientDocumentEmbedder\n",
        "from gradient_haystack.embedders.gradient_text_embedder import GradientTextEmbedder\n",
        "from gradient_haystack.generator.base import GradientGenerator\n",
        "from haystack import Document, Pipeline\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.in_memory.document_store import InMemoryDocumentStore\n",
        "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.builders.answer_builder import AnswerBuilder\n",
        "import os\n",
        "import requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4rNiS8FGPXd"
      },
      "outputs": [],
      "source": [
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"oaiYItHbvKffOP1hTUj1463fcv0jVVHR\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\"\n",
        "\n",
        "fine_tuned_Model_Id = \"9eb593b3-747e-403e-8f19-06113e4a9a6b_model_adapter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSljEN8TIEaK",
        "outputId": "1186dc4c-cdd6-4bee-f183-9f9de84195b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "75697\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.26it/s]\n"
          ]
        }
      ],
      "source": [
        "document_store = InMemoryDocumentStore()\n",
        "writer = DocumentWriter(document_store=document_store)\n",
        "\n",
        "\n",
        "document_embedder = GradientDocumentEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "# URL of the online repository where the Raw_Text_Data.txt file is located\n",
        "url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/Raw_Text_Data.txt\"\n",
        "\n",
        "# Send a GET request to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the contents of the downloaded file\n",
        "    text_data = response.text\n",
        "else:\n",
        "    # If the request was not successful, print an error message\n",
        "    print(\"Failed to download the file from the URL:\", url)\n",
        "\n",
        "docs = [\n",
        "    Document(content=text_data)\n",
        "]\n",
        "\n",
        "print(len(text_data))\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
        "indexing_pipeline.add_component(instance=writer, name=\"writer\")\n",
        "indexing_pipeline.connect(\"document_embedder\", \"writer\")\n",
        "indexing_pipeline.run({\"document_embedder\": {\"documents\": docs}})\n",
        "\n",
        "text_embedder = GradientTextEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "generator = GradientGenerator(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    model_adapter_id=fine_tuned_Model_Id,\n",
        "    max_generated_token_count=350,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FO4BvUHpIrxp"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"You are helpful assistant meant to answer questions relating to animal husbandry. Answer the query, based on the\n",
        "content in the documents. if you dont know the answer respond by saying you are unable to assist with that at the moment.\n",
        "{{documents}}\n",
        "Query: {{query}}\n",
        "\\nAnswer:\n",
        "\"\"\"\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
        "prompt_builder = PromptBuilder(template=prompt)\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(instance=text_embedder, name=\"text_embedder\")\n",
        "rag_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
        "rag_pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
        "rag_pipeline.add_component(instance=generator, name=\"generator\")\n",
        "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
        "rag_pipeline.connect(\"generator.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
        "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "\n",
        "def LLM_Run(question):\n",
        "    result = rag_pipeline.run(\n",
        "        {\n",
        "            \"text_embedder\": {\"text\": question},\n",
        "            \"prompt_builder\": {\"query\": question},\n",
        "            \"answer_builder\": {\"query\": question}\n",
        "        }\n",
        "    )\n",
        "    return result[\"answer_builder\"][\"answers\"][0].data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlb_oNaItUz",
        "outputId": "456fa38e-6012-4764-c41b-7fbe46f28d97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No, bulls do not show signs of Trichomoniasis. This sexually transmitted disease affects primarily cows and heifers.\n"
          ]
        }
      ],
      "source": [
        "Query = \"Do bulls show signs of Trichomoniasis?\"\n",
        "print(LLM_Run(Query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "TqhOWDwomTYv",
        "outputId": "2c294147-ba7d-4642-c200-5d7bec6d1e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: anvil-uplink in /usr/local/lib/python3.10/dist-packages (0.4.2)\n",
            "Collecting argparse (from anvil-uplink)\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.18.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (1.16.0)\n",
            "Requirement already satisfied: ws4py in /usr/local/lib/python3.10/dist-packages (from anvil-uplink) (0.5.1)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "b4d1db075ff04dbcb41871b91533c79e",
              "pip_warning": {
                "packages": [
                  "argparse"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!pip install anvil-uplink"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4_bgYoymwmX"
      },
      "outputs": [],
      "source": [
        "import anvil.server"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk11zvaqm6Gz",
        "outputId": "18b6f4fd-e870-4f66-8bd2-b9d63d0ace67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connecting to wss://anvil.works/uplink\n",
            "Anvil websocket open\n",
            "Connected to \"Default Environment\" as SERVER\n"
          ]
        }
      ],
      "source": [
        "anvil.server.connect(\"server_5A33FQHS5NVTR2BFISKONOLE-UOGPLND5WG64J4TQ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umew5Tuor32l"
      },
      "outputs": [],
      "source": [
        "@anvil.server.callable\n",
        "def question_answer(question):\n",
        "  response = LLM_Run(question)\n",
        "  print(response)\n",
        "  return response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "jzQEr0QXuSyp",
        "outputId": "93c8ac5e-90d8-4b04-fd37-a635eeec33b7"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-95cac3476493>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0manvil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_forever\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anvil/server.py\u001b[0m in \u001b[0;36mwait_forever\u001b[0;34m()\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "anvil.server.wait_forever()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyMahR1Wj2nK65ovj355tELd",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}