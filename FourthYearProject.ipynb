{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNkfjcqKv1bdDYzvgeR/IT8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swafey-karanja/Model-training/blob/main/FourthYearProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **VET-ASSISTANT LLM**"
      ],
      "metadata": {
        "id": "zeANMo5VPYYo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Installing dependencies**"
      ],
      "metadata": {
        "id": "zu96hl0nOwUk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un-Qup9eNRq3",
        "outputId": "b2c9d695-06d7-49bc-fd03-8bc7842dd381"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradientai\n",
            "  Downloading gradientai-1.8.0-py3-none-any.whl (296 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.5/296.5 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aenum>=3.1.11 (from gradientai)\n",
            "  Downloading aenum-3.1.15-py3-none-any.whl (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic<2.0.0,>=1.10.5 (from gradientai)\n",
            "  Downloading pydantic-1.10.14-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai) (2.0.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<2.0.0,>=1.10.5->gradientai) (4.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai) (1.16.0)\n",
            "Installing collected packages: aenum, pydantic, gradientai\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.6.3\n",
            "    Uninstalling pydantic-2.6.3:\n",
            "      Successfully uninstalled pydantic-2.6.3\n",
            "Successfully installed aenum-3.1.15 gradientai-1.8.0 pydantic-1.10.14\n",
            "Collecting langchain\n",
            "  Downloading langchain-0.1.11-py3-none-any.whl (807 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.28)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain)\n",
            "  Downloading langchain_community-0.0.26-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m77.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.29 (from langchain)\n",
            "  Downloading langchain_core-0.1.29-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.22-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.25.2)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.10.14)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading marshmallow-3.21.1-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (3.7.1)\n",
            "Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.29->langchain) (23.2)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.29->langchain) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: orjson, mypy-extensions, marshmallow, jsonpointer, typing-inspect, langsmith, jsonpatch, langchain-core, dataclasses-json, langchain-text-splitters, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.1.11 langchain-community-0.0.26 langchain-core-0.1.29 langchain-text-splitters-0.0.1 langsmith-0.1.22 marshmallow-3.21.1 mypy-extensions-1.0.0 orjson-3.9.15 typing-inspect-0.9.0\n",
            "Collecting gradient_haystack\n",
            "  Downloading gradient_haystack-0.4.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradient_haystack) (1.8.0)\n",
            "Collecting haystack-ai (from gradient_haystack)\n",
            "  Downloading haystack_ai-2.0.0b8-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.0/253.0 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (3.1.15)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (1.10.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack) (2.0.7)\n",
            "Collecting boilerpy3 (from haystack-ai->gradient_haystack)\n",
            "  Downloading boilerpy3-1.0.7-py3-none-any.whl (22 kB)\n",
            "Collecting haystack-bm25 (from haystack-ai->gradient_haystack)\n",
            "  Downloading haystack_bm25-1.0.2-py2.py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.1.3)\n",
            "Collecting lazy-imports (from haystack-ai->gradient_haystack)\n",
            "  Downloading lazy_imports-0.3.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (3.2.1)\n",
            "Collecting openai>=1.1.0 (from haystack-ai->gradient_haystack)\n",
            "  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (1.5.3)\n",
            "Collecting posthog (from haystack-ai->gradient_haystack)\n",
            "  Downloading posthog-3.5.0-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.3/41.3 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (6.0.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack) (4.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai>=1.1.0->haystack-ai->gradient_haystack)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai->gradient_haystack) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack) (2023.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack) (2.31.0)\n",
            "Collecting monotonic>=1.5 (from posthog->haystack-ai->gradient_haystack)\n",
            "  Downloading monotonic-1.6-py2.py3-none-any.whl (8.2 kB)\n",
            "Collecting backoff>=1.10.0 (from posthog->haystack-ai->gradient_haystack)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack) (2024.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack)\n",
            "  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai->gradient_haystack) (3.3.2)\n",
            "Installing collected packages: monotonic, lazy-imports, haystack-bm25, h11, boilerpy3, backoff, posthog, httpcore, httpx, openai, haystack-ai, gradient_haystack\n",
            "Successfully installed backoff-2.2.1 boilerpy3-1.0.7 gradient_haystack-0.4.0 h11-0.14.0 haystack-ai-2.0.0b8 haystack-bm25-1.0.2 httpcore-1.0.4 httpx-0.27.0 lazy-imports-0.3.1 monotonic-1.6 openai-1.13.3 posthog-3.5.0\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Collecting rouge\n",
            "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Installing collected packages: rouge\n",
            "Successfully installed rouge-1.0.1\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n",
            "Collecting wandb\n",
            "  Downloading wandb-0.16.4-py3-none-any.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb)\n",
            "  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb)\n",
            "  Downloading sentry_sdk-1.40.6-py2.py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.5/258.5 kB\u001b[0m \u001b[31m27.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb) (6.0.1)\n",
            "Collecting setproctitle (from wandb)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.42 docker-pycreds-0.4.0 gitdb-4.0.11 sentry-sdk-1.40.6 setproctitle-1.3.3 smmap-5.0.1 wandb-0.16.4\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16\n",
            "Requirement already satisfied: rouge in /usr/local/lib/python3.10/dist-packages (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from rouge) (1.16.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install gradientai --upgrade\n",
        "!pip install langchain\n",
        "!pip install -U gradient_haystack\n",
        "!pip install regex\n",
        "!pip install rouge\n",
        "!pip install nltk\n",
        "!pip install wandb\n",
        "!pip install datasets\n",
        "!pip install rouge\n",
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.Loading the Dataset**"
      ],
      "metadata": {
        "id": "Ids19xUiPP_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import requests\n",
        "\n",
        "# URL of the online repository where the dataset is hosted\n",
        "dataset_url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/data.json\"\n",
        "\n",
        "# Make an HTTP GET request to fetch the dataset\n",
        "response = requests.get(dataset_url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Load the dataset from the response content\n",
        "    train_dataset = json.loads(response.text)\n",
        "\n",
        "    # Print the size of the dataset\n",
        "    print(\"Dataset Size:\", len(train_dataset))\n",
        "else:\n",
        "    # Print an error message if the request failed\n",
        "    print(\"Failed to fetch dataset. Status code:\", response.status_code)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWjkVgavPkjM",
        "outputId": "30d02fb6-cb2a-4538-a265-0e1f377fb130"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Size: 1008\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Break the data into batches**"
      ],
      "metadata": {
        "id": "KnzGaOMVpGf4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def divide_into_Batches(number, chunk_size):  # Define a function to divide a number into chunks of a given size\n",
        "    Batches = []\n",
        "    while number > 0:\n",
        "        if number >= chunk_size:\n",
        "            Batches.append(chunk_size)\n",
        "            number -= chunk_size\n",
        "        else:\n",
        "            Batches.append(number)\n",
        "            break\n",
        "\n",
        "    return Batches\n",
        "\n",
        "Batches = divide_into_Batches(len(train_dataset), 100)  # Divide the dataset into chunks of 100 samples each\n",
        "print(\"Batches size\")\n",
        "print(Batches)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DYzINIs6pErW",
        "outputId": "ad593456-9b8a-4e5c-9208-21789141697b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batches size\n",
            "[100, 100, 100, 100, 100, 100, 100, 100, 100, 100, 8]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.Load the Base Model.**"
      ],
      "metadata": {
        "id": "apmpbpeppW2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the Nous-Hermes-Llama2-13b base model"
      ],
      "metadata": {
        "id": "I-W4W_s3pySl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"t4YxBtnw9hQ4qVB8ia9P4K8XkPLoDoZZ\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\""
      ],
      "metadata": {
        "id": "uPy0iC61pj1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from gradientai import Gradient\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "\n",
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"t4YxBtnw9hQ4qVB8ia9P4K8XkPLoDoZZ\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\"\n",
        "\n",
        "gradient =  Gradient()\n",
        "\n",
        "print(\"Available Base Models\")\n",
        "for i in gradient.list_models(only_base=True):\n",
        "    print(\"\\t\",i)\n",
        "\n",
        "base_model_id = \"NousResearch/Nous-Hermes-Llama2-13b\"\n",
        "base_model_name = \"nous-hermes2\"\n",
        "base_model = gradient.get_base_model(base_model_slug=\"nous-hermes2\") # base model Nous-Hermes-Llama2-13b\n",
        "\n",
        "print(\"\\nBase Model Chosen :\", base_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UOarEjaEpvWX",
        "outputId": "880009fc-16ba-42fb-a68c-28b64e5d4cd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Base Models\n",
            "\t <gradientai._base_model.BaseModel object at 0x78da99a219c0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x78da99a21990>\n",
            "\t <gradientai._base_model.BaseModel object at 0x78da99a22020>\n",
            "\t <gradientai._base_model.BaseModel object at 0x78da99a220e0>\n",
            "\t <gradientai._base_model.BaseModel object at 0x78da99a21cf0>\n",
            "\n",
            "Base Model Chosen : <gradientai._base_model.BaseModel object at 0x78da99a22170>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Creating a Model Adapter**\n",
        "\n",
        "* Adapters are small, lightweight modules inserted between\n",
        "existing layers of a pre-trained LLM. They act like \"add-ons\" that focus on learning task-specific information without modifying the core knowledge captured in the original model.\n",
        "\n",
        "* The addapter server as the object that we are going to fine tune"
      ],
      "metadata": {
        "id": "G2WHiZMvtxGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_finetune_model_name=\"Llama2-13b/Vet-assistant\"\n",
        "Fine_Tune__adapter = base_model.create_model_adapter(\n",
        "        name=our_finetune_model_name, # base mode nous hermis\n",
        "        learning_rate=0.00005, #  Determines how fast a model updates its knowledge during fine-tuning.\n",
        "        rank=8,  # Dimensionality\n",
        "\n",
        "    )\n",
        "\n",
        "# default hyperparameters Frozen\n",
        "hyperparameters = {\n",
        "                  \"block_size\": 1024,\n",
        "                  \"model_max_length\": 2048,\n",
        "                  \"padding\": \"right\",\n",
        "                  \"use_flash_attention_2\": False,\n",
        "                  \"disable_gradient_checkpointing\": False,\n",
        "                  \"logging_steps\": -1,\n",
        "                  \"evaluation_strategy\": \"epoch\",\n",
        "                  \"save_total_limit\": 1,\n",
        "                  \"save_strategy\": \"epoch\",\n",
        "                  \"auto_find_batch_size\": False,\n",
        "                  \"mixed_precision\": \"fp16\",\n",
        "                  \"epochs\": 3,\n",
        "                  \"batch_size\": 100,\n",
        "                  \"warmup_ratio\": 0.1,\n",
        "                  \"gradient_accumulation\": 1,\n",
        "                  \"optimizer\": \"adamw_torch\",\n",
        "                  \"scheduler\": \"linear\",\n",
        "                  \"weight_decay\": 0,\n",
        "                  \"max_grad_norm\": 1,\n",
        "                  \"seed\": 42,\n",
        "                  \"apply_chat_template\": False,\n",
        "                  \"quantization\": \"int4\",\n",
        "                  \"target_modules\": \"\",\n",
        "                  \"merge_adapter\": False,\n",
        "                  \"peft\": True,\n",
        "                  \"lora_r\": 16,\n",
        "                  \"lora_alpha\": 32,\n",
        "                  \"lora_dropout\": 0.05\n",
        "  }\n",
        "\n",
        "\n",
        "print(f\"Base model id                : {Fine_Tune__adapter._base_model_id}\")\n",
        "print(f\"Fine tune model Name         : { Fine_Tune__adapter.name}\")\n",
        "print(f\"Fine tune model adapter id   : {Fine_Tune__adapter.id}\")\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "print(\"Size of object in memory, in bytes.\", Fine_Tune__adapter.__format__.__sizeof__()) # Size of object in memory, in bytes.\n",
        "Fine_Tune__adapter.__dict__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKK8fwv5t_mX",
        "outputId": "a20557cb-f129-4600-f0e6-17e3c1528f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Base model id                : cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model\n",
            "Fine tune model Name         : Llama2-13b/Vet-assistant\n",
            "Fine tune model adapter id   : 640e55a6-3046-4824-8831-10e7ae54a1d6_model_adapter\n",
            "\n",
            "\n",
            "\n",
            "Size of object in memory, in bytes. 56\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'_api_instance': <gradientai.openapi.client.api.models_api.ModelsApi at 0x78da99a21630>,\n",
              " '_id': '640e55a6-3046-4824-8831-10e7ae54a1d6_model_adapter',\n",
              " '_workspace_id': 'c571b959-4ce8-474a-b3f3-398c7b347c57_workspace',\n",
              " '_async_semaphore': <asyncio.locks.Semaphore object at 0x78da99a22bc0 [unlocked, value:8]>,\n",
              " '_base_model_id': 'cc2dafce-9e6e-4a23-a918-cad6ba89e42e_base_ml_model',\n",
              " '_name': 'Llama2-13b/Vet-assistant'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **4. Fine Tuning The Adaptor**\n",
        "\n",
        "For this case we will be performing Laura-based finetuning which means that we are freezing about 99% of the layers and then finetuning an adapter on top of it.\n",
        "\n",
        "LoRA: Low-Rank Adaptation of Large Language Models is a novel technique introduced by Microsoft researchers to deal with the problem of fine-tuning large-language models with billions of parameters\n",
        "\n",
        " LoRA proposes to freeze pre-trained model weights and inject trainable layers (rank-decomposition matrices) in each transformer block. This greatly reduces the number of trainable parameters and GPU memory requirements since gradients don't need to be computed for most model weights\n",
        "\n",
        "\n",
        "why LoRA finetuning:\n",
        "\n",
        "1. Faster Training: Since only the added task-specific layers are trained while the pre-trained model's parameters remain frozen, the fine-tuning process is generally faster compared to training a model from scratch\n",
        "2. Computation requirements are lower. We could create a full fine-tuned model in a 2080 Ti with 11 GB of VRAM!\n",
        "3. Trained weights are  much smaller. Because the original model is frozen and we inject new layers to be trained\n",
        "\n",
        " [for more info](https://huggingface.co/blog/lora)"
      ],
      "metadata": {
        "id": "fPU-mOK0uzfF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Our Model id:  {Fine_Tune__adapter.id}\")\n",
        "num_epochs = 2  # num_epochs is the number of times you fine-tune the model # more epochs tends to get better results, but you also run the risk of \"overfitting\"\n",
        "count = 0\n",
        "print(\"================================================================\\n\")\n",
        "print(\"Fine tuning . . .\\n\")\n",
        "while count < num_epochs:\n",
        "    print(f\"Fine-tuning the model, iteration {count + 1}\")\n",
        "    s = 0\n",
        "    n = 1\n",
        "    for Batch in Batches:\n",
        "        print(f\"Batch {n} range: {s} : {(s + Batch)}\")\n",
        "\n",
        "        # Try to fine-tune the model with the chunk of samples,\n",
        "        while True:\n",
        "            try:\n",
        "                metric = Fine_Tune__adapter.fine_tune(samples=train_dataset[s: s + Batch])\n",
        "                print(f\"\\t Batch {n} Evaluation :\", metric)\n",
        "                break\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "\n",
        "\n",
        "        s += Batch\n",
        "        n += 1\n",
        "    count = count + 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JhFoAaE5vPAk",
        "outputId": "edf6ced0-64f4-4318-c5b9-0471ba6af4c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our Model id:  640e55a6-3046-4824-8831-10e7ae54a1d6_model_adapter\n",
            "================================================================\n",
            "\n",
            "Fine tuning . . .\n",
            "\n",
            "Fine-tuning the model, iteration 1\n",
            "Batch 1 range: 0 : 100\n",
            "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=18044.285\n",
            "Batch 2 range: 100 : 200\n",
            "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=11166.195\n",
            "Batch 3 range: 200 : 300\n",
            "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=10463.905\n",
            "Batch 4 range: 300 : 400\n",
            "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=11904.527\n",
            "Batch 5 range: 400 : 500\n",
            "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=12875.653\n",
            "Batch 6 range: 500 : 600\n",
            "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=17145.766\n",
            "Batch 7 range: 600 : 700\n",
            "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=17443.84\n",
            "Batch 8 range: 700 : 800\n",
            "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=17171.729\n",
            "Batch 9 range: 800 : 900\n",
            "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=11821.522\n",
            "Batch 10 range: 900 : 1000\n",
            "\t Batch 10 Evaluation : number_of_trainable_tokens=10560 sum_loss=9575.032\n",
            "Batch 11 range: 1000 : 1008\n",
            "\t Batch 11 Evaluation : number_of_trainable_tokens=1470 sum_loss=1383.5267\n",
            "Fine-tuning the model, iteration 2\n",
            "Batch 1 range: 0 : 100\n",
            "\t Batch 1 Evaluation : number_of_trainable_tokens=8007 sum_loss=7455.944\n",
            "Batch 2 range: 100 : 200\n",
            "\t Batch 2 Evaluation : number_of_trainable_tokens=8043 sum_loss=6490.472\n",
            "Batch 3 range: 200 : 300\n",
            "\t Batch 3 Evaluation : number_of_trainable_tokens=9278 sum_loss=6915.945\n",
            "Batch 4 range: 300 : 400\n",
            "\t Batch 4 Evaluation : number_of_trainable_tokens=10454 sum_loss=8198.889\n",
            "Batch 5 range: 400 : 500\n",
            "\t Batch 5 Evaluation : number_of_trainable_tokens=13087 sum_loss=9883.227\n",
            "Batch 6 range: 500 : 600\n",
            "\t Batch 6 Evaluation : number_of_trainable_tokens=16332 sum_loss=11200.203\n",
            "Batch 7 range: 600 : 700\n",
            "\t Batch 7 Evaluation : number_of_trainable_tokens=18421 sum_loss=13736.754\n",
            "Batch 8 range: 700 : 800\n",
            "\t Batch 8 Evaluation : number_of_trainable_tokens=18931 sum_loss=10428.556\n",
            "Batch 9 range: 800 : 900\n",
            "\t Batch 9 Evaluation : number_of_trainable_tokens=13457 sum_loss=6267.5444\n",
            "Batch 10 range: 900 : 1000\n",
            "\t Batch 10 Evaluation : number_of_trainable_tokens=10560 sum_loss=6038.368\n",
            "Batch 11 range: 1000 : 1008\n",
            "\t Batch 11 Evaluation : number_of_trainable_tokens=1470 sum_loss=548.53864\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **5.Model Inference**\n",
        "\n",
        "\"model inference\" typically refers to the process of using a trained model to make predictions on new, unseen data. Fine-tuning a model involves taking a pre-trained model and further training it on a specific task or dataset to improve its performance.\n",
        "\n",
        "When fine-tuning a model, the process of inference remains the same as with any other trained model. Once the fine-tuning is complete, you can use the model to make predictions on new data by passing the data through the model and obtaining the output."
      ],
      "metadata": {
        "id": "JH8ghczL_Xy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import LLMChain    # Import the LLMChain class for building LLM-based workflows\n",
        "from langchain.llms import GradientLLM   # Import the GradientLLM class for interacting with Gradient AI's API\n",
        "from langchain.prompts import PromptTemplate # Import the PromptTemplate class for defining how to prompt the LLM\n",
        "import gradientai\n",
        "import os # Import the os module for potential file system interactions"
      ],
      "metadata": {
        "id": "fsGJw4hw_8O5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Fine_Tune__adapter_ID = \"640e55a6-3046-4824-8831-10e7ae54a1d6_model_adapter\"\n",
        "#Fine_Tune__adapter_ID = Fine_Tune__adapter.id\n",
        "#  creating a GradientLLM object\n",
        "llm = GradientLLM(\n",
        "    model=Fine_Tune__adapter_ID,\n",
        "    model_kwargs=dict(\n",
        "        max_generated_token_count=128, # Adjust how your model generates completions\n",
        "        temperature = 0.7, # randomness\n",
        "        top_k=50 # Restricts the model to pick from k most likely words,\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "3Vbg5iMnAA1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Formatting Prompts**\n",
        "\n",
        "The model follows the Alpaca prompt format which provides a structured way to input information to the model, guiding it on what kind of output is desired."
      ],
      "metadata": {
        "id": "vVCG5MR7Au16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template = \"\"\"### Instruction: {Instruction} \\n\\n### Response:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"Instruction\"])\n",
        "\n",
        "llm_chain = LLMChain(prompt=prompt, llm=llm)"
      ],
      "metadata": {
        "id": "fzevoH6vBLXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### *Example* *inputs* *and* *corresponding* *outputs*"
      ],
      "metadata": {
        "id": "Vg0GwPNHBmjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Question1 = \"How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\"\n",
        "Question2 = \"what are some of the risk factors associated with lameness in dairy cows?\"\n",
        "Question3 = \"How dose lameness impact mill production, reproduction and overall well-being in cows\"\n",
        "Question4 = \"What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \"\n",
        "Question5 = \"what specific health managment strategies should be implemented to prevent or treat common cow diseases?\"\n",
        "\n",
        "\n",
        "print(\"Question :\\n\", Question1)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question1}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question2)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question2}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question3)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question3}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question4)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question4}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n",
        "\n",
        "print(\"Question :\\n\", Question5)\n",
        "Answer = llm_chain.run(Instruction=f\"{Question5}\")\n",
        "print(\"Answer :\\n\", Answer, \"\\n\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcTdHlRZBcN7",
        "outputId": "1bf9617b-6c27-41da-8000-200a88b7b93c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question :\n",
            " How does Clostridial Diseases impact milk production, reproductive performance, and overall well-being in cows?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
            "  warn_deprecated(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer :\n",
            " Clostridial Diseases can negatively impact milk production, reproductive performance, and overall well-being in cows. Symptoms can include diarrhea, reduced milk yield, and reproductive issues such as abortion. Early detection and treatment are critical for minimizing the impact on milk production and overall cow health. \n",
            "\n",
            "\n",
            "Question :\n",
            " what are some of the risk factors associated with lameness in dairy cows?\n",
            "Answer :\n",
            " Risk factors associated with lameness in dairy cows include poor hygiene, inadequate foot care, excessive weight, improper teat positioning, and uneven weight distribution. Management strategies such as routine foot trimming, maintaining proper hygiene, and minimizing stress contribute to reducing the risk of lameness in dairy cows. \n",
            "\n",
            "\n",
            "Question :\n",
            " How dose lameness impact mill production, reproduction and overall well-being in cows\n",
            "Answer :\n",
            " Lameness in cows can impact mill production by decreasing overall efficiency. Reproduction may be negatively affected as lame cows may experience difficulty in mating. Overall well-being is compromised due to the pain and discomfort associated with lameness. Implementing proper foot care measures, early detection, and timely treatment contribute to minimizing the impact of lameness on cows. \n",
            "\n",
            "\n",
            "Question :\n",
            " What diseases are prevelant in dairy small ruminant, and what managment practice can mitigate their impact \n",
            "Answer :\n",
            " Diseases prevalent in dairy small ruminants include mastitis, metabolic disorders, and infectious diseases. Implementing proper hygiene, adhering to a balanced feeding program, and promptly addressing any signs of illness can help mitigate their impact. Strategies such as regular health checks, early detection, and timely intervention contribute to reducing the prevalence of diseases and promoting overall herd health in dairy small ruminants. \n",
            "\n",
            "\n",
            "Question :\n",
            " what specific health managment strategies should be implemented to prevent or treat common cow diseases?\n",
            "Answer :\n",
            " Implementing specific health management strategies, such as vaccination programs, regular health checks, and early intervention in cases of illness, can prevent or treat common cow diseases and promote overall herd health. \n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **6.Model Evaluation**\n",
        "\n",
        "Here we are using two popular automatic evaluation metrics to assess the performance of your LLM:\n",
        "\n",
        "* BLEU score: This metric calculates the n-gram precision between the generated response and the reference response\n",
        "\n",
        "* ROUGE score: This metric measures the overlap in word n-grams and longest common subsequences between the generated response and the reference response.\n",
        "\n",
        "BLEU and ROUGE scores are calculated to compare the generated response with the target response."
      ],
      "metadata": {
        "id": "sCh6YPY-DPh8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import json\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from rouge import Rouge\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.llms import GradientLLM\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os"
      ],
      "metadata": {
        "id": "MxVdOqQJEZNS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def compute_rouge_scores(hypotheses, references):\n",
        "    rouge = Rouge()\n",
        "    scores = rouge.get_scores(hypotheses, references, avg=True)\n",
        "    return scores\n",
        "\n",
        "\n",
        "def compute_bleu_score(target_response, llm_responses):\n",
        "    bleu_score = corpus_bleu([target_response.split()], [llm_responses.split()])  # Calculate BLEU score\n",
        "    return bleu_score\n",
        "\n",
        "\n",
        "def Find_Instruction(input_pattern, input_string):\n",
        "    matches = re.findall(input_pattern, input_string, re.DOTALL)\n",
        "\n",
        "    # If there are matches, extract the first one\n",
        "    extracted_string = None\n",
        "    if matches:\n",
        "        extracted_string = matches[0]\n",
        "\n",
        "    return extracted_string\n",
        "\n",
        "\n",
        "def Evaluate(Sample=None, count=0):\n",
        "    print(\"\\n =================================== Evaluation =================================== \")\n",
        "    input_pattern = r'<s>### Instruction:\\n(.*?) \\n'\n",
        "    response_pattern = r'Response:\\n(.*?)</s>'\n",
        "    bleu_scoreS = []\n",
        "    rouge_scoreS = []\n",
        "\n",
        "    if count != 0:\n",
        "        iteration = count - 1\n",
        "    else:\n",
        "        iteration = count\n",
        "\n",
        "    while iteration >= 0:\n",
        "\n",
        "        input_query = Find_Instruction(input_pattern, Sample[iteration][\"inputs\"])\n",
        "        target_response = Find_Instruction(response_pattern, Sample[iteration][\"inputs\"])\n",
        "\n",
        "        if input_query and target_response is not None:\n",
        "            print(\"\\n ---------------------------------------------------------------\")\n",
        "            print(\"INPUT QUERY:\\n\", input_query)\n",
        "            print(\"\\nTARGET RESPONSE:\\n\", target_response)\n",
        "\n",
        "            llm_responses = llm_chain.run(Instruction=f\"{input_query}\")\n",
        "            print(\"\\nLLM RESPONSE:\\n\", llm_responses)\n",
        "\n",
        "            rouge_scores = compute_rouge_scores(llm_responses, target_response)\n",
        "\n",
        "            bleu_score = compute_bleu_score(target_response, llm_responses)\n",
        "            print(\"\\nBLEU Score:\", bleu_score)\n",
        "            print(\"ROUGE Scores:\")\n",
        "            print(\"\\tROUGE-1 F1 Score:\", rouge_scores[\"rouge-1\"][\"f\"])\n",
        "            print(\"\\tROUGE-2 F1 Score:\", rouge_scores[\"rouge-2\"][\"f\"])\n",
        "            print(\"\\tROUGE-L F1 Score:\", rouge_scores[\"rouge-l\"][\"f\"])\n",
        "            rouge_scoreS.append((rouge_scores[\"rouge-1\"][\"f\"], rouge_scores[\"rouge-2\"][\"f\"], rouge_scores[\"rouge-l\"][\"f\"]))\n",
        "            bleu_scoreS.append(bleu_score)\n",
        "\n",
        "\n",
        "        iteration -= 1\n",
        "\n",
        "    if count > 0:\n",
        "        rouge_scores1 = 0\n",
        "        rouge_scores2 = 0\n",
        "        rouge_scores3 = 0\n",
        "        bleu_scoreA = 0\n",
        "\n",
        "        for i in bleu_scoreS:\n",
        "            bleu_scoreA += i\n",
        "        for i in rouge_scoreS:\n",
        "            rouge_scores1 += i[0]\n",
        "            rouge_scores2 += i[1]\n",
        "            rouge_scores3 += i[2]\n",
        "\n",
        "        print(\"\\nAverageBLEU Score:\", bleu_scoreA)\n",
        "        print(f\"Average ROUGE Scores for {count} samples\")\n",
        "        print(\"\\tAverage ROUGE-1 F1 Score:\", rouge_scores1 / count)\n",
        "        print(\"\\tAverage ROUGE-2 F1 Score:\", rouge_scores2 / count)\n",
        "        print(\"\\tAverageROUGE-L F1 Score:\", rouge_scores3 / count)\n",
        "\n",
        "    print(\"\\n ---------------------------------------------------------------\")\n",
        "\n",
        "\n",
        "Evaluate(Sample=train_dataset, count=3)  # one sample evaluation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHfeZ8FfEbXp",
        "outputId": "cd742ba7-927a-4ad6-8e06-cbf3089e485f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " =================================== Evaluation =================================== \n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " How does technology contribute to advancements in animal husbandry?\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Technology in animal husbandry includes innovations like automated feeding systems, precision breeding techniques, and health monitoring devices. These advancements enhance efficiency, reduce costs, and improve overall management practices.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Technology contributes to advancements in animal husbandry by enhancing precision in feeding management, optimizing breeding practices, and improving overall efficiency in resource utilization.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.4166666617447917\n",
            "\tROUGE-2 F1 Score: 0.08163264811328642\n",
            "\tROUGE-L F1 Score: 0.37499999507812504\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Elaborate on the challenges faced in modern animal husbandry practices.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Modern animal husbandry faces challenges such as disease management, ethical concerns, and environmental impact. Balancing productivity with animal welfare and sustainability is an ongoing challenge for practitioners in the field.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Modern animal husbandry faces challenges related to disease management, environmental sustainability, and meeting consumer demands for ethical and humane practices. Consequently, innovations in technology and genetics contribute to addressing these challenges to ensure efficient and responsible animal husbandry.\n",
            "\n",
            "BLEU Score: 0\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.41379309845422124\n",
            "\tROUGE-2 F1 Score: 0.1538461489041422\n",
            "\tROUGE-L F1 Score: 0.3793103398335315\n",
            "\n",
            " ---------------------------------------------------------------\n",
            "INPUT QUERY:\n",
            " Discuss the role of nutrition in animal husbandry.\n",
            "\n",
            "TARGET RESPONSE:\n",
            " Nutrition plays a crucial role in animal husbandry as it directly impacts the health, growth, and productivity of livestock. Properly balanced diets ensure optimal development and efficient utilization of nutrients for various purposes, such as milk and meat production.\n",
            "\n",
            "LLM RESPONSE:\n",
            " Nutrition plays a crucial role in animal husbandry, affecting overall health, growth rates, and productivity. Proper nutrition ensures optimal development and performance in various production systems.\n",
            "\n",
            "BLEU Score: 8.06798322521923e-232\n",
            "ROUGE Scores:\n",
            "\tROUGE-1 F1 Score: 0.47457626636024136\n",
            "\tROUGE-2 F1 Score: 0.28571428092718576\n",
            "\tROUGE-L F1 Score: 0.47457626636024136\n",
            "\n",
            "AverageBLEU Score: 8.06798322521923e-232\n",
            "Average ROUGE Scores for 3 samples\n",
            "\tAverage ROUGE-1 F1 Score: 0.4350120088530847\n",
            "\tAverage ROUGE-2 F1 Score: 0.1737310259815381\n",
            "\tAverageROUGE-L F1 Score: 0.4096288670906327\n",
            "\n",
            " ---------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 2-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 3-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n",
            "/usr/local/lib/python3.10/dist-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
            "The hypothesis contains 0 counts of 4-gram overlaps.\n",
            "Therefore the BLEU score evaluates to 0, independently of\n",
            "how many N-gram overlaps of lower order it contains.\n",
            "Consider using lower n-gram order or use SmoothingFunction()\n",
            "  warnings.warn(_msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " ## **7. Intergrating  Retreival-Augmented Generation**"
      ],
      "metadata": {
        "id": "0oBkpjyuFZlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradient_haystack==0.2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4f4FrLhGJ7V",
        "outputId": "e047f5b1-e7cd-42cb-d219-ccb50247a14b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradient_haystack==0.2.0\n",
            "  Downloading gradient_haystack-0.2.0-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: gradientai>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from gradient_haystack==0.2.0) (1.8.0)\n",
            "Requirement already satisfied: haystack-ai in /usr/local/lib/python3.10/dist-packages (from gradient_haystack==0.2.0) (2.0.0b8)\n",
            "Requirement already satisfied: aenum>=3.1.11 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (3.1.15)\n",
            "Requirement already satisfied: pydantic<2.0.0,>=1.10.5 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (1.10.14)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.8.2)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from gradientai>=1.4.0->gradient_haystack==0.2.0) (2.0.7)\n",
            "Requirement already satisfied: boilerpy3 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.0.7)\n",
            "Requirement already satisfied: haystack-bm25 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.0.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.1.3)\n",
            "Requirement already satisfied: lazy-imports in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (0.3.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (10.1.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.2.1)\n",
            "Requirement already satisfied: openai>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.13.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (1.5.3)\n",
            "Requirement already satisfied: posthog in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (3.5.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (6.0.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (8.2.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.66.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from haystack-ai->gradient_haystack==0.2.0) (4.10.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.7.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.27.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->gradientai>=1.4.0->gradient_haystack==0.2.0) (1.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from haystack-bm25->haystack-ai->gradient_haystack==0.2.0) (1.25.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->haystack-ai->gradient_haystack==0.2.0) (2.1.5)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->haystack-ai->gradient_haystack==0.2.0) (2023.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.7 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.31.0)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from posthog->haystack-ai->gradient_haystack==0.2.0) (2.2.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (3.6)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.2.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (2024.2.2)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (1.0.4)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.1.0->haystack-ai->gradient_haystack==0.2.0) (0.14.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.7->posthog->haystack-ai->gradient_haystack==0.2.0) (3.3.2)\n",
            "Installing collected packages: gradient_haystack\n",
            "  Attempting uninstall: gradient_haystack\n",
            "    Found existing installation: gradient-haystack 0.4.0\n",
            "    Uninstalling gradient-haystack-0.4.0:\n",
            "      Successfully uninstalled gradient-haystack-0.4.0\n",
            "Successfully installed gradient_haystack-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gradient_haystack.embedders.gradient_document_embedder import GradientDocumentEmbedder\n",
        "from gradient_haystack.embedders.gradient_text_embedder import GradientTextEmbedder\n",
        "from gradient_haystack.generator.base import GradientGenerator\n",
        "from haystack import Document, Pipeline\n",
        "from haystack.components.writers import DocumentWriter\n",
        "from haystack.document_stores.in_memory.document_store import InMemoryDocumentStore\n",
        "from haystack.components.retrievers.in_memory.embedding_retriever import InMemoryEmbeddingRetriever\n",
        "from haystack.components.builders import PromptBuilder\n",
        "from haystack.components.builders.answer_builder import AnswerBuilder\n",
        "import os\n",
        "import requests"
      ],
      "metadata": {
        "id": "kE3KAEc6GLVK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ['GRADIENT_ACCESS_TOKEN'] = \"t4YxBtnw9hQ4qVB8ia9P4K8XkPLoDoZZ\"\n",
        "os.environ['GRADIENT_WORKSPACE_ID'] = \"c571b959-4ce8-474a-b3f3-398c7b347c57_workspace\"\n",
        "\n",
        "fine_tuned_Model_Id = \"640e55a6-3046-4824-8831-10e7ae54a1d6_model_adapter\""
      ],
      "metadata": {
        "id": "x4rNiS8FGPXd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_store = InMemoryDocumentStore()\n",
        "writer = DocumentWriter(document_store=document_store)\n",
        "\n",
        "\n",
        "document_embedder = GradientDocumentEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "# URL of the online repository where the Raw_Text_Data.txt file is located\n",
        "url = \"https://raw.githubusercontent.com/swafey-karanja/Model-training/main/Raw_Text_Data.txt\"\n",
        "\n",
        "# Send a GET request to download the file\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    # Read the contents of the downloaded file\n",
        "    text_data = response.text\n",
        "else:\n",
        "    # If the request was not successful, print an error message\n",
        "    print(\"Failed to download the file from the URL:\", url)\n",
        "\n",
        "docs = [\n",
        "    Document(content=text_data)\n",
        "]\n",
        "\n",
        "indexing_pipeline = Pipeline()\n",
        "indexing_pipeline.add_component(instance=document_embedder, name=\"document_embedder\")\n",
        "indexing_pipeline.add_component(instance=writer, name=\"writer\")\n",
        "indexing_pipeline.connect(\"document_embedder\", \"writer\")\n",
        "indexing_pipeline.run({\"document_embedder\": {\"documents\": docs}})\n",
        "\n",
        "text_embedder = GradientTextEmbedder(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        ")\n",
        "\n",
        "generator = GradientGenerator(\n",
        "    access_token=os.environ[\"GRADIENT_ACCESS_TOKEN\"],\n",
        "    workspace_id=os.environ[\"GRADIENT_WORKSPACE_ID\"],\n",
        "    model_adapter_id=fine_tuned_Model_Id,\n",
        "    max_generated_token_count=350,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HSljEN8TIEaK",
        "outputId": "8fc9934b-3183-4bba-e2e6-4b1ab4f25b6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.03it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"You are helpful assistant ment to answer questions relating to animal husbandry. Answer the query, based on the\n",
        "content in the documents. if you dont know the answer say you don't know.\n",
        "{{documents}}\n",
        "Query: {{query}}\n",
        "\\nAnswer:\n",
        "\"\"\"\n",
        "\n",
        "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
        "prompt_builder = PromptBuilder(template=prompt)\n",
        "\n",
        "rag_pipeline = Pipeline()\n",
        "rag_pipeline.add_component(instance=text_embedder, name=\"text_embedder\")\n",
        "rag_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
        "rag_pipeline.add_component(instance=prompt_builder, name=\"prompt_builder\")\n",
        "rag_pipeline.add_component(instance=generator, name=\"generator\")\n",
        "rag_pipeline.add_component(instance=AnswerBuilder(), name=\"answer_builder\")\n",
        "rag_pipeline.connect(\"generator.replies\", \"answer_builder.replies\")\n",
        "rag_pipeline.connect(\"retriever\", \"answer_builder.documents\")\n",
        "rag_pipeline.connect(\"text_embedder\", \"retriever\")\n",
        "rag_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
        "rag_pipeline.connect(\"prompt_builder\", \"generator\")\n",
        "\n",
        "\n",
        "def LLM_Run(question):\n",
        "    result = rag_pipeline.run(\n",
        "        {\n",
        "            \"text_embedder\": {\"text\": question},\n",
        "            \"prompt_builder\": {\"query\": question},\n",
        "            \"answer_builder\": {\"query\": question}\n",
        "        }\n",
        "    )\n",
        "    return result[\"answer_builder\"][\"answers\"][0].data"
      ],
      "metadata": {
        "id": "FO4BvUHpIrxp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Query = \"what is the best way of preventing bacterial infections?\"\n",
        "print(LLM_Run(Query))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXlb_oNaItUz",
        "outputId": "e17437d9-1c9d-4397-c2bd-53910ef0ef50"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preventing bacterial infections involves maintaining proper hygiene, minimizing stress factors, and implementing vaccination programs. Regular health checks, prompt treatment of any signs of infection, and maintaining optimal environmental conditions contribute to reducing the risk of bacterial infections in animal husbandry.\n"
          ]
        }
      ]
    }
  ]
}